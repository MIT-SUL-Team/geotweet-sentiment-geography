{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Graph by Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: @SirenaYu\n",
    "\n",
    "This notebook is divided into 2 sections:\n",
    "* Section 1 is data exploration, in which we identify the top 10 cities for each country covered by Twitter geography data.\n",
    "* Section 2 is graph generation, in which time series graphs from 2015 to 2021 is generated for each region.\n",
    "\n",
    "This is a little messy, to be organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from script import days_in_month\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime\n",
    "from datetime import date as dt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2015/1/10, 0AM Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_path = \"/srv/data/twitter_geography/2015/\"\n",
    "sent_path = \"/srv/data/twitter_sentiment/2015/\"\n",
    "\n",
    "with gzip.open(''.join([geo_path, \"geography_2015_1_10_00.csv.gz\"])) as f:\n",
    "    geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "\n",
    "with gzip.open(''.join([sent_path, \"bert_sentiment_2015_1_10_00.csv.gz\"])) as f:\n",
    "    sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "    \n",
    "common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "\n",
    "common_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_posts[\"NAME_0\"].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_us_cities = common_posts[common_posts[\"NAME_0\"] == \"United States\"][\"NAME_2\"].value_counts().nlargest(5).index.tolist()\n",
    "top_5_us_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_to_sent_score = dict()\n",
    "\n",
    "for city in top_5_us_cities:\n",
    "    avg_sent_score = common_posts[common_posts[\"NAME_2\"] == city][\"score\"].mean()\n",
    "    city_to_sent_score[city] = avg_sent_score\n",
    "        \n",
    "city_to_sent_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2015/1/10, All Day Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the daily average of top 5 US cities, we do the following:\n",
    "\n",
    "* For each hour of the day, keep track of the top 10 US cities and their average sentiment scores / number of posts.\n",
    "* Sum up number of posts and calculate top 5 US cities of that day.\n",
    "* Using average sentiment scores / number of posts from each hour, calculate a weighted average, resulting in an accurate daily average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_path = \"/srv/data/twitter_geography/2015/\"\n",
    "sent_path = \"/srv/data/twitter_sentiment/2015/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hour in range(0, 1):\n",
    "    with gzip.open(''.join([geo_path, \"geography_2015_1_10_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "        geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "    with gzip.open(''.join([sent_path, \"bert_sentiment_2015_1_10_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "        sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "    common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_posts[\"STATE_CITY\"] = common_posts['NAME_1'].astype(str) + '_' + common_posts['NAME_2']\n",
    "common_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_posts[common_posts[\"STATE_CITY\"]==\"Arizona_Maricopa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_path = \"/srv/data/twitter_geography/2015/\"\n",
    "sent_path = \"/srv/data/twitter_sentiment/2015/\"\n",
    "\n",
    "hour_2_city_2_avg_n_numpost = dict()\n",
    "city_2_numpost = dict()\n",
    "city_avg_score_numpost_df = None\n",
    "\n",
    "for hour in range(0, 24):\n",
    "    with gzip.open(''.join([geo_path, \"geography_2015_1_10_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "        geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "    with gzip.open(''.join([sent_path, \"bert_sentiment_2015_1_10_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "        sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "    common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "    top_10_us_cities = common_posts[common_posts[\"NAME_0\"] == \"United States\"][\"NAME_2\"].value_counts().nlargest(10).index.tolist()\n",
    "    city_2_avg_n_numpost = dict()\n",
    "    post_in_cities = common_posts[common_posts[\"NAME_2\"].isin(top_10_us_cities)]\n",
    "    city_result = post_in_cities.groupby([\"NAME_2\"]).agg({\"score\": np.sum, \"message_id\": len}).reset_index()\n",
    "    city_result.rename(columns={\"NAME_2\": \"city\", \"score\": \"total_score\", \"message_id\": \"num_posts\"}, inplace=True)\n",
    "    if city_avg_score_numpost_df is None:\n",
    "        city_avg_score_numpost_df = city_result\n",
    "    else:\n",
    "        city_avg_score_numpost_df = city_avg_score_numpost_df.merge(city_result, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "        city_avg_score_numpost_df[\"total_score\"] = city_avg_score_numpost_df[\"total_score_x\"].fillna(0) + city_avg_score_numpost_df[\"total_score_y\"].fillna(0)\n",
    "        city_avg_score_numpost_df[\"num_posts\"] = city_avg_score_numpost_df[\"num_posts_x\"].fillna(0) + city_avg_score_numpost_df[\"num_posts_y\"].fillna(0)\n",
    "        city_avg_score_numpost_df.drop(columns=[\"total_score_x\", \"total_score_y\", \"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "    hour_2_city_2_avg_n_numpost[hour] = city_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_avg_score_numpost_df = city_avg_score_numpost_df.sort_values(by=['num_posts'], ascending=False).reset_index(drop=True)[:5]\n",
    "city_avg_score_numpost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_avg_score_numpost_df[\"daily_avg_score\"] = city_avg_score_numpost_df[\"total_score\"]/city_avg_score_numpost_df[\"num_posts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_avg_score_numpost_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_us_cities = [k for k, v in sorted(city_2_numpost.items(), key=lambda item: item[1], reverse=True)][:5]\n",
    "top_5_us_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_2_avg = dict()\n",
    "\n",
    "for city in top_5_us_cities:\n",
    "    for hour in range(0, 24):\n",
    "        try: \n",
    "            hour_avg, hour_numpost = hour_2_city_2_avg_n_numpost[hour][city]\n",
    "            weight_avg = hour_avg * hour_numpost / city_2_numpost[city]\n",
    "            if city not in city_2_avg.keys():\n",
    "                city_2_avg[city] = weight_avg\n",
    "            else:\n",
    "                city_2_avg[city] += weight_avg\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "city_2_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_sent_avg_n_numpost_top_m_cities(country, m, year, month, day):\n",
    "    \"\"\"\n",
    "    @param country: str\n",
    "    @param m: int, top m cities that we want to look at\n",
    "    @param year: int\n",
    "    @param month: int\n",
    "    @param day: int\n",
    "    \n",
    "    return: dict, maps city to (daily_sent_avg, daily_numpost)\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    geo_path = \"\".join([\"/srv/data/twitter_geography/\", str(year), \"/\"])\n",
    "    sent_path = \"\".join([\"/srv/data/twitter_sentiment/\", str(year), \"/\"])\n",
    "    date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "\n",
    "    \n",
    "    result_df = None\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        pre_open_time = datetime.now()\n",
    "        try:\n",
    "            with gzip.open(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        try:\n",
    "            with gzip.open(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        \n",
    "        common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "        \n",
    "        top_2m_cities = common_posts[common_posts[\"NAME_0\"] == country][\"NAME_2\"].value_counts().nlargest(2*m).index.tolist()\n",
    "        post_in_cities = common_posts[common_posts[\"NAME_2\"].isin(top_2m_cities)]\n",
    "        city_result = post_in_cities.groupby([\"NAME_2\"]).agg({\"score\": np.sum, \"message_id\": len}).reset_index()\n",
    "        city_result.rename(columns={\"NAME_2\": \"city\", \"score\": \"total_score\", \"message_id\": \"num_posts\"}, inplace=True)\n",
    "        if result_df is None:\n",
    "            result_df = city_result\n",
    "        else:\n",
    "            result_df = result_df.merge(city_result, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "            result_df[\"total_score\"] = result_df[\"total_score_x\"].fillna(0) + result_df[\"total_score_y\"].fillna(0)\n",
    "            result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "            result_df.drop(columns=[\"total_score_x\", \"total_score_y\", \"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "        \n",
    "    result_df = result_df.sort_values(by=['num_posts'], ascending=False).reset_index(drop=True)[:m]\n",
    "    result_df[\"daily_avg_score\"] = result_df[\"total_score\"]/result_df[\"num_posts\"]\n",
    "    result_df.drop(columns=[\"total_score\"], inplace=True)\n",
    "    end_time = datetime.now()\n",
    "    print(\"get_daily_sent_avg_n_numpost_top_m_cities took\", end_time-start_time, \"seconds.\")\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_daily_sent_avg_n_numpost_top_m_cities(\"United States\", 5, 2015, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passed test case 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_sent_avg_n_numpost_top_m_cities(\"United States\", 15, 2016, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_sent_avg_n_numpost_top_m_cities(\"United Kingdom\", 15, 2016, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_sent_avg_n_numpost_top_m_cities(\"United States\", 15, 2019, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019/1, Daily Average, United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = None\n",
    "\n",
    "for year in range(2019, 2020):\n",
    "    for month in range(1, 2):\n",
    "        for day in range(1, days_in_month(month, year)+1):\n",
    "            date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "            daily_df = get_daily_sent_avg_n_numpost_top_m_cities(\"United States\", 15, year, month, day)\n",
    "            print(date)\n",
    "            daily_df = daily_df.rename(columns={\"daily_avg_score\": date})\n",
    "            display(daily_df)\n",
    "            if result_df is None:\n",
    "                result_df = daily_df\n",
    "            else:\n",
    "                result_df = result_df.merge(daily_df, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "                result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "                result_df.drop(columns=[\"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "            display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = result_df.sort_values(by=['num_posts'], ascending=False).reset_index(drop=True)[:5]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.drop(columns=[\"num_posts\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = result_df.T\n",
    "graph_df.rename(columns=graph_df.iloc[0], inplace=True)\n",
    "graph_df.drop(graph_df.index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in graph_df.columns.to_list():\n",
    "    # df = pd.read_csv(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2019_1.csv\"]))\n",
    "    base = dt(2019, 1, 1)\n",
    "    numdays = 31\n",
    "    x = [base + timedelta(days=x) for x in range(numdays)]\n",
    "    y = graph_df[city]\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.title(\"\".join([\"Daily Average Sentiment of Posts in \", city, \" County, 2019-1\"]))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel(\"Dates\")\n",
    "    plt.ylabel(\"Daily Average Sentiment\")\n",
    "    # plt.legend(bbox_to_anchor=(1.6, 1.0), loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2019_1_graph.png\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019, Daily Average, United States "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_sent_avg_n_numpost(country, cities, year, month, day):\n",
    "    \"\"\"\n",
    "    @param county: str, desire country\n",
    "    @param cities: list of str, cities that we want to look at\n",
    "    @param year: int\n",
    "    @param month: int\n",
    "    @param day: int\n",
    "    \n",
    "    return: dict, maps city to (daily_sent_avg, daily_numpost)\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    geo_path = \"\".join([\"/srv/data/twitter_geography/\", str(year), \"/\"])\n",
    "    sent_path = \"\".join([\"/srv/data/twitter_sentiment/\", str(year), \"/\"])\n",
    "    date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "    \n",
    "    result_df = None\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        pre_open_time = datetime.now()\n",
    "        try:\n",
    "            with gzip.open(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        try:\n",
    "            with gzip.open(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        \n",
    "        common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "        \n",
    "        post_in_cities = common_posts[common_posts[\"NAME_2\"].isin(cities)]\n",
    "        city_result = post_in_cities.groupby([\"NAME_2\"]).agg({\"score\": np.sum, \"message_id\": len}).reset_index()\n",
    "        city_result.rename(columns={\"NAME_2\": \"city\", \"score\": \"total_score\", \"message_id\": \"num_posts\"}, inplace=True)\n",
    "        if result_df is None:\n",
    "            result_df = city_result\n",
    "        else:\n",
    "            result_df = result_df.merge(city_result, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "            result_df[\"total_score\"] = result_df[\"total_score_x\"].fillna(0) + result_df[\"total_score_y\"].fillna(0)\n",
    "            result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "            result_df.drop(columns=[\"total_score_x\", \"total_score_y\", \"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "    \n",
    "    if result_df is None:\n",
    "        result_data = np.array([\n",
    "            cities,\n",
    "            [0] * len(cities),\n",
    "            [0] * len(cities)\n",
    "        ])\n",
    "        result_df = pd.DataFrame(data=result_data.T,\n",
    "                                columns=[\"city\", \"num_posts\", \"daily_avg_score\"])\n",
    "        result_df = result_df.astype({'num_posts': 'int64', 'daily_avg_score': 'int64'})\n",
    "    else:\n",
    "        result_df[\"daily_avg_score\"] = result_df[\"total_score\"]/result_df[\"num_posts\"]\n",
    "        result_df.drop(columns=[\"total_score\"], inplace=True)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(\"get_daily_sent_avg_n_numpost took\", end_time-start_time, \"seconds.\")\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(2019, 2020):\n",
    "    for month in range(1, 13):\n",
    "        result_df = None\n",
    "        for day in range(1, days_in_month(month, year)+1):\n",
    "            date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "            daily_df = get_daily_sent_avg_n_numpost(\"United States\", cities, year, month, day)\n",
    "            daily_df = daily_df.rename(columns={\"daily_avg_score\": date})\n",
    "            if result_df is None:\n",
    "                result_df = daily_df\n",
    "            else:\n",
    "                result_df = result_df.merge(daily_df, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "                # print(day, result_df.dtypes)\n",
    "                result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "                result_df.drop(columns=[\"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "        result_df.drop(columns=[\"num_posts\"], inplace=True)\n",
    "        graph_df = result_df.T\n",
    "        graph_df.rename(columns=graph_df.iloc[0], inplace=True)\n",
    "        graph_df.drop(graph_df.index[0], inplace=True)\n",
    "        graph_df.to_csv(\"\".join([\"../output/sentiment_graph_by_region/us_top5_cities_2019_\", str(month), \".csv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = None\n",
    "\n",
    "for month in range(1, 13):\n",
    "    month_df = pd.read_csv(\"\".join([\"../output/sentiment_graph_by_region/us_top5_cities_2020_\", str(month), \".csv\"]))\n",
    "    month_df = month_df.rename(columns={\"Unnamed: 0\":\"date\"})\n",
    "    if year_df is None:\n",
    "        year_df = month_df\n",
    "    else:\n",
    "        year_df = pd.concat([year_df, month_df], ignore_index=True)\n",
    "\n",
    "year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2020, 2021):\n",
    "    for month in range(1, 13):\n",
    "        result_df = None\n",
    "        for day in range(1, days_in_month(month, year)+1):\n",
    "            date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "            daily_df = get_daily_sent_avg_n_numpost(\"United States\", cities, year, month, day)\n",
    "            daily_df = daily_df.rename(columns={\"daily_avg_score\": date})\n",
    "            if result_df is None:\n",
    "                result_df = daily_df\n",
    "            else:\n",
    "                result_df = result_df.merge(daily_df, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "                # print(day, result_df.dtypes)\n",
    "                result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "                result_df.drop(columns=[\"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "        result_df.drop(columns=[\"num_posts\"], inplace=True)\n",
    "        graph_df = result_df.T\n",
    "        graph_df.rename(columns=graph_df.iloc[0], inplace=True)\n",
    "        graph_df.drop(graph_df.index[0], inplace=True)\n",
    "        graph_df.to_csv(\"\".join([\"../output/sentiment_graph_by_region/us_top5_cities_2020_\", str(month), \".csv\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_us_cities = ['Los Angeles', 'Harris', 'Cook', 'New York', 'Orange']\n",
    "\n",
    "for city in top_5_us_cities:\n",
    "    # df = pd.read_csv(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2019_1.csv\"]))\n",
    "    base = dt(2020, 1, 1)\n",
    "    numdays = 366\n",
    "    x = [base + timedelta(days=x) for x in range(numdays)]\n",
    "    y = year_df[city]\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.title(\"\".join([\"Daily Average Sentiment of Posts in \", city, \" County, 2020\"]))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel(\"Dates\")\n",
    "    plt.ylabel(\"Daily Average Sentiment\")\n",
    "    # plt.legend(bbox_to_anchor=(1.6, 1.0), loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2020_graph.png\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_daily_avg_df_by_year(country, cities, year):\n",
    "    \"\"\"\n",
    "    @param cities: list of str, list of cities to generate \n",
    "    \"\"\"\n",
    "    result_df = None\n",
    "    for month in range(1, 13):\n",
    "        for day in range(1, days_in_month(month, year)+1):\n",
    "            date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "            daily_df = get_daily_sent_avg_n_numpost(\"United States\", cities, year, month, day)\n",
    "            daily_df = daily_df.rename(columns={\"daily_avg_score\": date})\n",
    "            if result_df is None:\n",
    "                result_df = daily_df\n",
    "            else:\n",
    "                result_df = result_df.merge(daily_df, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "                result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "                result_df.drop(columns=[\"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "    result_df.drop(columns=[\"num_posts\"], inplace=True)\n",
    "    graph_df = result_df.T\n",
    "    graph_df.rename(columns=graph_df.iloc[0], inplace=True)\n",
    "    graph_df.drop(graph_df.index[0], inplace=True)\n",
    "    graph_df.to_csv(\"\".join([\"../output/sentiment_graph_by_region/\"] + cities + [\"_year\"]))\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_us_cities = ['Los Angeles', 'Harris', 'Cook', 'New York', 'Orange']\n",
    "\n",
    "get_daily_sent_avg_n_numpost(\"United States\", top_5_us_cities, 2019, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_us_2019_df = generate_daily_avg_df_by_year(\"United States\", top_5_us_cities, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing not good data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_files_2019_geo = pd.read_csv(\"../output/missing_file_report/corrupted_files_2019_geography.csv\")\n",
    "\n",
    "corrupted_files_2019_sent = pd.read_csv(\"../output/missing_file_report/corrupted_files_2019_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_files_2019_geo.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "corrupted_files_2019_sent.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(file_name, category):\n",
    "    if category == \"geography\":\n",
    "        return \"_\".join(file_name.split(\"/\")[5].split(\".\")[0].split(\"_\")[1:4])\n",
    "    else:\n",
    "        return \"_\".join(file_name.split(\"/\")[5].split(\".\")[0].split(\"_\")[2:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_files_2019_geo['dates'] = corrupted_files_2019_geo['corrupted_files'].apply(lambda x: get_date(x, \"geography\"))\n",
    "corrupted_files_2019_sent['dates'] = corrupted_files_2019_sent['corrupted_files'].apply(lambda x: get_date(x, \"sentiment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_dates = set(corrupted_files_2019_sent['dates'].unique()) | set(corrupted_files_2019_geo['dates'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_dates.add('2019_11_16') # to be fixed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = None\n",
    "\n",
    "for month in range(1, 13):\n",
    "    month_df = pd.read_csv(\"\".join([\"../output/sentiment_graph_by_region/us_top5_cities_2019_\", str(month), \".csv\"]))\n",
    "    month_df = month_df.rename(columns={\"Unnamed: 0\":\"date\"})\n",
    "    if year_df is None:\n",
    "        year_df = month_df\n",
    "    else:\n",
    "        year_df = pd.concat([year_df, month_df], ignore_index=True)\n",
    "\n",
    "year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_us_cities = ['Los Angeles', 'Harris', 'Cook', 'New York', 'Orange']\n",
    "\n",
    "for city in top_5_us_cities:\n",
    "    year_df[city] = np.where(year_df['date'].isin(corrupted_dates), np.nan, year_df[city])\n",
    "\n",
    "# year_df[\"Cook\"] = np.where(year_df['date'].isin(corrupted_dates), np.nan, year_df[\"Cook\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in top_5_us_cities:\n",
    "    # df = pd.read_csv(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2019_1.csv\"]))\n",
    "    base = dt(2020, 1, 1)\n",
    "    numdays = 365\n",
    "    x = [base + timedelta(days=x) for x in range(numdays)]\n",
    "    y = year_df[city]\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    plt.title(\"\".join([\"Daily Average Sentiment of Posts in \", city, \" County, 2019\"]))\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xlabel(\"Dates\")\n",
    "    plt.ylabel(\"Daily Average Sentiment\")\n",
    "    # plt.legend(bbox_to_anchor=(1.6, 1.0), loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "    # plt.savefig(\"\".join([\"../output/sentiment_graph_by_region/\", city, \"_2019_graph.png\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
