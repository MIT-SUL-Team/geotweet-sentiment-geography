{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Graph Generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook refactors the code in sentiment_graph_by_region to fit new needs. The notebook does the following:\n",
    "\n",
    "1) Function to generate sentiment score csv files by month.\n",
    "2) Function to generate sentiment graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from script import days_in_month\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from datetime import timedelta, datetime\n",
    "from datetime import date as dt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_daily_sent_avg_and_num_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_sentiment_avg_and_num_posts(cities, year, month, day):\n",
    "    \"\"\"\n",
    "    @param cities: list of str in the form of \"COUNTRY_STATE_CITY\" (ex. \"United States_California_Los Angeles\"), cities that we want to look at\n",
    "    @param year: int\n",
    "    @param month: int\n",
    "    @param day: int\n",
    "    \n",
    "    returns: Pandas DataFrame, contains three columns: city, daily_avg_score (average sentiment scores from the day) and num_posts (number of posts from the day)\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    geo_path = \"\".join([\"/srv/data/twitter_geography/\", str(year), \"/\"])\n",
    "    sent_path = \"\".join([\"/srv/data/twitter_sentiment/\", str(year), \"/\"])\n",
    "    date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "    \n",
    "    result_df = None\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        pre_open_time = datetime.now()\n",
    "        try:\n",
    "            with gzip.open(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([geo_path, \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        try:\n",
    "            with gzip.open(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])) as f:\n",
    "                sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "        except FileNotFoundError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"does not exist.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(''.join([sent_path, \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"]), \"is empty.\")\n",
    "            continue\n",
    "        \n",
    "        common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "        \n",
    "        common_posts[\"COUNTRY_STATE_CITY\"] = common_posts['NAME_0'].astype(str) + '_' + common_posts['NAME_1'].astype(str) + '_' + common_posts['NAME_2']\n",
    "        \n",
    "        post_in_cities = common_posts[common_posts[\"COUNTRY_STATE_CITY\"].isin(cities)]\n",
    "        city_result = post_in_cities.groupby([\"COUNTRY_STATE_CITY\"]).agg({\"score\": np.sum, \"message_id\": len}).reset_index()\n",
    "        city_result.rename(columns={\"COUNTRY_STATE_CITY\": \"city\", \"score\": \"total_score\", \"message_id\": \"num_posts\"}, inplace=True)\n",
    "        if result_df is None:\n",
    "            result_df = city_result\n",
    "        else:\n",
    "            result_df = result_df.merge(city_result, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "            result_df[\"total_score\"] = result_df[\"total_score_x\"].fillna(0) + result_df[\"total_score_y\"].fillna(0)\n",
    "            result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "            result_df.drop(columns=[\"total_score_x\", \"total_score_y\", \"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "    \n",
    "    if result_df is None:\n",
    "        result_data = np.array([\n",
    "            cities,\n",
    "            [0] * len(cities),\n",
    "            [np.nan] * len(cities)\n",
    "        ])\n",
    "        result_df = pd.DataFrame(data=result_data.T,\n",
    "                                columns=[\"city\", \"num_posts\", \"daily_avg_score\"])\n",
    "        result_df = result_df.astype({'num_posts': 'int64'})\n",
    "    else:\n",
    "        result_df[\"daily_avg_score\"] = result_df[\"total_score\"]/result_df[\"num_posts\"]\n",
    "        result_df.drop(columns=[\"total_score\"], inplace=True)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(\"get_daily_sent_avg_and_num_posts\", end_time-start_time, \"seconds.\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"United States_California_Los Angeles\", \"United States_Illinois_Cook\"]\n",
    "\n",
    "get_daily_sentiment_avg_and_num_posts(cities, 2019, 11, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_sentiment_avg_and_num_posts(cities, 2019, 11, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_sentiment_avg_and_num_posts(cities, 2019, 11, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_daily_sentiment_avg_and_num_posts_by_month_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_daily_sentiment_avg_and_num_posts_by_month_csv(cities, year, month, city_group, out_dir):\n",
    "    \"\"\"\n",
    "    @param cities: list of str in the form of \"COUNTRY_STATE_CITY\" (ex. \"United States_California_Los Angeles\"), cities that we want to look at\n",
    "    @param year: int, desired year\n",
    "    @param month: int, desired month\n",
    "    @param city_group: name of group of the cities (ex. \"top_5_us_cities\")\n",
    "    @param out_dir: str, output directory\n",
    "    \n",
    "    Generates csv files for average daily sentiment and daily number of posts for given year and month and cities. \n",
    "    Save csv file under the name \"CITY_GROUP_sentiment_avg_YEAR_MONTH.csv\" or \"CITY_GROUP_num_posts_YEAR_MONTH.csv\"\n",
    "    \n",
    "    returns: nothing\n",
    "    \"\"\"\n",
    "    sentiment_avg_df = None\n",
    "    num_post_df = None\n",
    "    for day in range(1, days_in_month(month, year)+1):\n",
    "        date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "        daily_df = get_daily_sentiment_avg_and_num_posts(cities, year, month, day)\n",
    "        daily_sentiment_avg_df = daily_df.drop(columns=[\"num_posts\"])\n",
    "        daily_sentiment_avg_df = daily_sentiment_avg_df.rename(columns={\"daily_avg_score\": date})\n",
    "        daily_num_post_df = daily_df.drop(columns=[\"daily_avg_score\"])\n",
    "        daily_num_post_df = daily_num_post_df.rename(columns={\"num_posts\": date})\n",
    "        if sentiment_avg_df is None:\n",
    "            sentiment_avg_df = daily_sentiment_avg_df\n",
    "        else:\n",
    "            sentiment_avg_df = sentiment_avg_df.merge(daily_sentiment_avg_df, on=\"city\", how=\"outer\")\n",
    "        if num_post_df is None:\n",
    "            num_post_df = daily_num_post_df\n",
    "        else:\n",
    "            num_post_df = num_post_df.merge(daily_num_post_df, on=\"city\", how=\"outer\")\n",
    "            \n",
    "    sentiment_avg_by_month_df = sentiment_avg_df.T\n",
    "    sentiment_avg_by_month_df.rename(columns=sentiment_avg_by_month_df.iloc[0], inplace=True)\n",
    "    sentiment_avg_by_month_df.drop(sentiment_avg_by_month_df.index[0], inplace=True)\n",
    "    display(sentiment_avg_by_month_df)\n",
    "    sentiment_avg_by_month_df.to_csv(\"\".join([out_dir, city_group, \"_sentiment_avg_\", str(year),\"_\", str(month).zfill(2), \".csv\"]))\n",
    "    \n",
    "    num_post_by_month_df = num_post_df.T\n",
    "    num_post_by_month_df.rename(columns=num_post_by_month_df.iloc[0], inplace=True)\n",
    "    num_post_by_month_df.drop(num_post_by_month_df.index[0], inplace=True)\n",
    "    display(num_post_by_month_df)\n",
    "    num_post_by_month_df.to_csv(\"\".join([out_dir, city_group, \"_num_posts_\", str(year),\"_\", str(month).zfill(2), \".csv\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = [\"United States_California_Los Angeles\", \"United States_Illinois_Cook\"]\n",
    "\n",
    "generate_daily_sentiment_avg_and_num_posts_by_month_csv(cities, 2019, 11, \"test_refactor_code\", \"../output/sentiment_graph_by_region/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../output/sentiment_graph_by_region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_sentiment_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_in_between(start, end):\n",
    "    \"\"\"\n",
    "    @param start: datetime object, start date\n",
    "    @param end: datetime object, end date\n",
    "    \n",
    "    returns: months in between the start and the end as a list (Ex. [\"2008_08\", \"2008_09\"])\n",
    "    \"\"\"\n",
    "    delta = end - start  # as timedelta\n",
    "    months_with_duplicate = [f\"{start + timedelta(days=i):%Y_%m}\" for i in range(delta.days + 1)]\n",
    "    months = pd.Series(months_with_duplicate).unique()\n",
    "    return months\n",
    "\n",
    "start_date = datetime(2008, 8, 1)\n",
    "end_date = datetime(2008, 9, 3)\n",
    "    \n",
    "print(date_range(start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrupted_dates(start, end, file_dir):\n",
    "    \"\"\"\n",
    "    @param start: datetime object, start date\n",
    "    @param end: datetime object, end date\n",
    "    @param file_dir: file directory under which corrupted file reports are stored\n",
    "    \n",
    "    returns: corrupted dates in between the start and the end as a list\n",
    "    \"\"\"\n",
    "    def get_date(file_name, category):\n",
    "        if category == \"geography\":\n",
    "            return \"_\".join(file_name.split(\"/\")[5].split(\".\")[0].split(\"_\")[1:4])\n",
    "        else:\n",
    "            return \"_\".join(file_name.split(\"/\")[5].split(\".\")[0].split(\"_\")[2:5])\n",
    "    \n",
    "    corrupted_dates = set()\n",
    "    for year in range(start.year, end.year+1):\n",
    "        try:\n",
    "            corrupted_files_geo = pd.read_csv(\"\".join([file_dir, \"corrupted_files_\", str(year), \"_geography.csv\"]))\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(\"\".join([\"Corrupted file report for geography data in \", year, \" does not exist\"]))\n",
    "        try:\n",
    "            corrupted_files_sent = pd.read_csv(\"\".join([file_dir, \"corrupted_files_\", str(year), \"_sentiment.csv\"]))\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(\"\".join([\"Corrupted file report for geography data in \", year, \" does not exist\"]))\n",
    "        \n",
    "        corrupted_files_geo.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "        corrupted_files_sent.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "        \n",
    "        corrupted_files_geo['dates'] = corrupted_files_geo['corrupted_files'].apply(lambda x: get_date(x, \"geography\"))\n",
    "        corrupted_files_sent['dates'] = corrupted_files_sent['corrupted_files'].apply(lambda x: get_date(x, \"sentiment\"))\n",
    "        \n",
    "        corrupted_dates_year = set(corrupted_files_sent['dates'].unique()) | set(corrupted_files_geo['dates'].unique())\n",
    "        corrupted_dates.update(corrupted_dates_year)\n",
    "    \n",
    "    return corrupted_dates\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_daily_sentiment_graph(start_date, end_date, city_group, in_dir, out_dir, remove_corrupted_data=True, corrupt_data_dir=None):\n",
    "    \"\"\"\n",
    "    @param start_date: datetime object\n",
    "    @param end_date: datetime object\n",
    "    @param city_group: str, used for searching for corresponding city_group csv files\n",
    "    @param in_dir: directory under which the sentiment score / num post by month csv files are stored \n",
    "    @param out_dir: directory to which the daily sentiment graphs will be stored\n",
    "    @param remove_corrupted_data: boolean, True if we want to remove corrupted data points, False if not\n",
    "    @param corrupt_data_dir: directory under which the corrupt data is \n",
    "    \n",
    "    Generates sentiment avaerage graphs from cities in city_group from the month of start_date to the month of end_date \n",
    "    and save it to out_dir\n",
    "    \"\"\"\n",
    "    # generating a dataframe that covers all months between start date and end date\n",
    "    all_df = None\n",
    "    \n",
    "    months = months_in_between(start_date, end_date)\n",
    "    for month in months:\n",
    "        try:\n",
    "            month_df = pd.read_csv(\"\".join([in_dir, city_group, \"_sentiment_avg_\", month, \".csv\"]))\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(\"\".join([\"csv file for \", month, \" in \", city_group, \" does not exist\"]))\n",
    "        month_df = month_df.rename(columns={\"Unnamed: 0\":\"date\"})\n",
    "        if all_df is None:\n",
    "            all_df = month_df\n",
    "        else:\n",
    "            all_df = pd.concat([all_df, month_df], ignore_index=True)\n",
    "    \n",
    "    cities = list(all_df.columns)\n",
    "    cities.remove(\"date\")\n",
    "    \n",
    "    # remove corrupted data points\n",
    "    if remove_corrupted_data:\n",
    "        if corrupt_data_dir is None:\n",
    "            raise Exception(\"Need to provide corrupt data directory as input.\")\n",
    "        corrupted_dates = get_corrupted_dates(start_date, end_date, corrupt_data_dir)\n",
    "        for city in cities:\n",
    "            all_df[city] = np.where(all_df['date'].isin(corrupted_dates), np.nan, all_df[city])\n",
    "    \n",
    "    # pdf = matplotlib.backends.backend_pdf.PdfPages(\"\".join([out_dir, city_group, \"_daily_sentiment_\", \n",
    "                                                            # months[0], \"_to_\",  months[len(months)-1], \".pdf\"]))\n",
    "    \n",
    "    for city in cities:\n",
    "        base = datetime(start_date.year, start_date.month, 1)\n",
    "        numdays = (datetime(end_date.year, end_date.month, days_in_month(end_date.month, end_date.year)) - base).days + 1\n",
    "        x = [base + timedelta(days=x) for x in range(numdays)]\n",
    "        y = all_df[city]\n",
    "\n",
    "        plt.plot(x, y)\n",
    "\n",
    "        plt.title(\"\".join([\"Daily Average Sentiment of Posts in \", city]))\n",
    "        plt.xticks(rotation = 45)\n",
    "        plt.xlabel(\"Dates\")\n",
    "        plt.ylabel(\"Daily Average Sentiment\")\n",
    "        # plt.legend(bbox_to_anchor=(1.6, 1.0), loc='upper right')\n",
    "\n",
    "        plt.show()\n",
    "        plt.savefig(\"\".join([out_dir, city,\"_daily_sentiment_\", months[0], \"_to_\",  months[len(months)-1], \".png\"]), bbox_inches = 'tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_d = datetime(2019, 11, 1)\n",
    "end_d = datetime(2019, 11, 1)\n",
    "\n",
    "generate_daily_sentiment_graph(start_d, end_d, \"test_refactor_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corrupted_dates(start_d, end_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
