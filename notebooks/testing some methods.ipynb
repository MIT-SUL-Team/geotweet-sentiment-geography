{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from datetime import timedelta\n",
    "from script import days_in_month, hours_in_day\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_SENTIMENT = \"/srv/data/twitter_sentiment/\"\n",
    "DIR_GEOGRAPHY = \"/srv/data/twitter_geography/\"\n",
    "DIR_STORE = \"./store/\"\n",
    "DIR_OUTPUT = \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_num_post_and_sentiment_and_missing_file(year, month, day):\n",
    "    \"\"\"\n",
    "\n",
    "    @param year:\n",
    "    @param month:\n",
    "    @param day:\n",
    "    @return:\n",
    "    1) result_df: Pandas dataframe, a dataframe that includes the following columns: year, month, day, country, state, city, num_posts, avg_sent_score\n",
    "    2) missing_files: a list of str, file names that do not exist\n",
    "    3) empty_files: a list of str, files names that are empty\n",
    "    4) file_name_to_num_post: dict, maps file names to number of posts in that file\n",
    "    \"\"\"\n",
    "    date = \"\".join([str(year), \"_\", str(month), \"_\", str(day).zfill(2)])\n",
    "    result_df = None # sentiment score and num post dataframe\n",
    "\n",
    "    # num_post and sentiment result\n",
    "    num_posts_by_city = pd.DataFrame() # pd dataframe to represent number of posts by city\n",
    "\n",
    "    # missing data result\n",
    "    missing_files = []\n",
    "    empty_files = []\n",
    "    \n",
    "    # getting file name to num post map for corrupt files later\n",
    "    file_name_to_num_post = dict()\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        has_error = False\n",
    "        try:\n",
    "            file_name = ''.join([DIR_GEOGRAPHY, str(year), \"/\" \"geography_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])\n",
    "            with gzip.open(file_name) as f:\n",
    "                geo_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "                file_name_to_num_post[file_name] = len(geo_posts)\n",
    "        except FileNotFoundError:\n",
    "            print(file_name, \"does not exist.\")\n",
    "            missing_files.append(file_name)\n",
    "            has_error = True\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(file_name, \"is empty.\")\n",
    "            empty_files.append(file_name)\n",
    "            has_error = True\n",
    "        try:\n",
    "            file_name = ''.join([DIR_SENTIMENT, str(year), \"/\", \"bert_sentiment_\", date, \"_\", str(hour).zfill(2), \".csv.gz\"])\n",
    "            with gzip.open(file_name) as f:\n",
    "                sent_posts = pd.read_csv(f, sep=\"\\t\")\n",
    "                file_name_to_num_post[file_name] = len(sent_posts)\n",
    "        except FileNotFoundError:\n",
    "            print(file_name, \"does not exist.\")\n",
    "            missing_files.append(file_name)\n",
    "            has_error = True\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(file_name, \"is empty.\")\n",
    "            empty_files.append(file_name)\n",
    "            has_error = True\n",
    "            \n",
    "        if has_error:\n",
    "            continue\n",
    "\n",
    "        common_posts = pd.merge(geo_posts, sent_posts, on=\"message_id\", how=\"inner\")\n",
    "\n",
    "        common_posts[\"COUNTRY_STATE_CITY\"] = common_posts['NAME_0'].astype(str) + '_' + common_posts['NAME_1'].astype(\n",
    "            str) + '_' + common_posts['NAME_2']\n",
    "\n",
    "        # num posts calculation\n",
    "        num_posts_by_city_this_hour = common_posts.groupby([\"COUNTRY_STATE_CITY\"]).size().to_frame().transpose()\n",
    "        num_posts_by_city = pd.concat([num_posts_by_city, num_posts_by_city_this_hour], join=\"outer\",\n",
    "                                         sort=True)\n",
    "\n",
    "        city_result = common_posts.groupby([\"COUNTRY_STATE_CITY\"]).agg(\n",
    "            {\"score\": np.sum, \"message_id\": len}).reset_index()\n",
    "        city_result.rename(columns={\"COUNTRY_STATE_CITY\": \"city\", \"score\": \"total_score\", \"message_id\": \"num_posts\"},\n",
    "                           inplace=True)\n",
    "        if result_df is None:\n",
    "            result_df = city_result\n",
    "        else:\n",
    "            result_df = result_df.merge(city_result, on=\"city\", how=\"outer\", suffixes=('_x', '_y'))\n",
    "            result_df[\"total_score\"] = result_df[\"total_score_x\"].fillna(0) + result_df[\"total_score_y\"].fillna(0)\n",
    "            result_df[\"num_posts\"] = result_df[\"num_posts_x\"].fillna(0) + result_df[\"num_posts_y\"].fillna(0)\n",
    "            result_df.drop(columns=[\"total_score_x\", \"total_score_y\", \"num_posts_x\", \"num_posts_y\"], inplace=True)\n",
    "\n",
    "    if result_df is None:\n",
    "        result_data = pd.DataFrame()\n",
    "    else:\n",
    "        result_df[\"year\"] = [year] * len(result_df)\n",
    "        result_df[\"month\"] = [month] * len(result_df)\n",
    "        result_df[\"day\"] = [day] * len(result_df)\n",
    "        result_df[\"num_posts\"] = result_df[\"num_posts\"].astype(int)\n",
    "        result_df[\"country\"] = result_df[\"city\"].apply(lambda x: x.split(\"_\")[0])\n",
    "        result_df[\"state\"] = result_df[\"city\"].apply(lambda x: \"_\".join(x.split(\"_\")[0:2]))\n",
    "        result_df[\"daily_avg_score\"] = result_df[\"total_score\"] / result_df[\"num_posts\"]\n",
    "        result_df.drop(columns=[\"total_score\"], inplace=True)\n",
    "        column_names = [\"year\", \"month\", \"day\", \"country\", \"state\", \"city\", \"num_posts\", \"daily_avg_score\"]\n",
    "        result_df = result_df[column_names]\n",
    "\n",
    "    return result_df, missing_files, empty_files, file_name_to_num_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daily_num_post_and_sentiment_and_missing_file(2012, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_method(year):\n",
    "    missing_files = []\n",
    "    empty_files = []\n",
    "    result_df = pd.DataFrame()\n",
    "    file_name_to_num_post = dict()\n",
    "    for month in range(7, 8):\n",
    "        for day in range(1, 3):\n",
    "            day_df, day_missing_files, day_empty_files, day_file_name_to_num_post = get_daily_num_post_and_sentiment_and_missing_file(year, month, day)\n",
    "            missing_files = missing_files + day_missing_files\n",
    "            empty_files = empty_files + day_empty_files\n",
    "            result_df = pd.concat([result_df, day_df], ignore_index=True)\n",
    "            file_name_to_num_post.update(day_file_name_to_num_post)\n",
    "            \n",
    "    bottom_10_percentile = pd.Series(data=list(file_name_to_num_post.values())).quantile(0.1)\n",
    "    print(list(file_name_to_num_post.values()))\n",
    "    print(bottom_10_percentile)\n",
    "    threshold = min(10000, bottom_10_percentile)\n",
    "    \n",
    "    corrupted_files = []\n",
    "    for file_name in file_name_to_num_post.keys():\n",
    "        if file_name_to_num_post[file_name] < threshold:\n",
    "            print(\"yeet\")\n",
    "            corrupted_files.append(file_name)\n",
    "    return result_df, missing_files, empty_files, corrupted_files\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = some_method(2014)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_10_percentile = a.groupby(by=[\"year\", \"month\", \"day\"]).sum()[\"num_posts\"].quantile(0.1)\n",
    "threshold = min(10000, bottom_10_percentile)\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 10, 100, 1000]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = {\"a\": 1, \"b\": 10, \"c\": 100, \"d\": 1000}\n",
    "list(hi.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017',\n",
       " '2018',\n",
       " '2012',\n",
       " '2015',\n",
       " '2021',\n",
       " '2016',\n",
       " '2019',\n",
       " '2014',\n",
       " '2020',\n",
       " '2013']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DIR_GEOGRAPHY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
